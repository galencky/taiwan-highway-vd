{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/galenchen/taiwan-highway-vd-project?scriptVersionId=126148967\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"84c7bf50","metadata":{"papermill":{"duration":0.005617,"end_time":"2023-04-17T14:26:26.222204","exception":false,"start_time":"2023-04-17T14:26:26.216587","status":"completed"},"tags":[]},"source":["# Download multiple DATE x VDID loop\n","## Set parameters."]},{"cell_type":"code","execution_count":1,"id":"7ef184f9","metadata":{"execution":{"iopub.execute_input":"2023-04-17T14:26:26.233335Z","iopub.status.busy":"2023-04-17T14:26:26.232773Z","iopub.status.idle":"2023-04-17T14:26:26.246915Z","shell.execute_reply":"2023-04-17T14:26:26.245967Z"},"papermill":{"duration":0.022744,"end_time":"2023-04-17T14:26:26.249395","exception":false,"start_time":"2023-04-17T14:26:26.226651","status":"completed"},"tags":[]},"outputs":[],"source":["#***********************************************************************\n","clear_working_folder = True # so you will only be mailed with the latest results, but make sure the files have been saved.\n","use_all_subfolders = False # if this is true, then you can only use Kaggle dataset.\n","realtime = True #will be set to false if you use all subfolders\n","use_yesterday = True # if true, the date_list will be changed to yesterday for scheduled runs.\n","\n","dataset_list = ['vd-2023-0326-0401'] # If the length of this list is longer than one, use_all_subfolders == True, realtime == False.\n","date_list = ['20230401']#[yesterday_str] if you want to run data from yesterday\n","vdid_list = [\"VD-N1-S-369.007-M-Loop\", \"VD-N1-S-369.400-M-Loop\", \"VD-N1-S-370.000-M-Loop\", \"VD-N1-S-371.010-M-Loop\"]\n","#vdid_list = [\"VD-N1-S-370.000-M-Loop\", \"VD-N1-S-371.010-M-Loop\"]\n","start_time = \"0000\"\n","end_time = \"2359\"\n","\n","your_email = 'galen147258369@gmail.com'\n","#***********************************************************************"]},{"cell_type":"markdown","id":"74992422","metadata":{"papermill":{"duration":0.004104,"end_time":"2023-04-17T14:26:26.258134","exception":false,"start_time":"2023-04-17T14:26:26.25403","status":"completed"},"tags":[]},"source":["## Load the dataset and the packages\n","* Suggest uploading the xml.gz files onto Kaggle so it doesn't fuck up the output storage.\n","* Also, the package xmltodict have to be installed before anything happens."]},{"cell_type":"code","execution_count":2,"id":"53aeda69","metadata":{"execution":{"iopub.execute_input":"2023-04-17T14:26:26.268957Z","iopub.status.busy":"2023-04-17T14:26:26.26849Z","iopub.status.idle":"2023-04-17T14:26:38.980804Z","shell.execute_reply":"2023-04-17T14:26:38.979307Z"},"papermill":{"duration":12.721163,"end_time":"2023-04-17T14:26:38.98386","exception":false,"start_time":"2023-04-17T14:26:26.262697","status":"completed"},"scrolled":true,"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting xmltodict\r\n","  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\r\n","Installing collected packages: xmltodict\r\n","Successfully installed xmltodict-0.13.0\r\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n","\u001b[0m"]}],"source":["!pip install xmltodict # Install xmltodict package if not already installed\n","\n","import xmltodict # Import xmltodict module for parsing XML data\n","import numpy as np # Import numpy and pandas for data analysis and processing\n","import pandas as pd\n","import json # Import json module for working with JSON data\n","from tqdm import tqdm # Import tqdm module for progress bars\n","import urllib.request # Import urllib.request module for working with URLs\n","import gzip # Import gzip module for working with compressed data files\n","from datetime import datetime, timedelta \n","import pytz # Import datetime and pytz modules for working with dates and times\n","import os # Import os module for working with the file system\n","\n","# Walk through the /kaggle/input directory and print out the path of each file in it\n","#for dirname, _, filenames in os.walk('/kaggle/input'):\n","    #for filename in filenames:\n","        #print(os.path.join(dirname, filename))"]},{"cell_type":"markdown","id":"03b3dc2b","metadata":{"papermill":{"duration":0.004674,"end_time":"2023-04-17T14:26:38.994144","exception":false,"start_time":"2023-04-17T14:26:38.98947","status":"completed"},"tags":[]},"source":["## Realtime downloading and conversion pipeline\n","\n","Download xml.gz file, decompress into xml file, then delete the xml.gz, then feed the xml into get_vd, then delete the xml after data is analyzed."]},{"cell_type":"code","execution_count":3,"id":"4ebdd63c","metadata":{"execution":{"iopub.execute_input":"2023-04-17T14:26:39.006119Z","iopub.status.busy":"2023-04-17T14:26:39.005645Z","iopub.status.idle":"2023-04-17T14:26:39.019858Z","shell.execute_reply":"2023-04-17T14:26:39.018136Z"},"papermill":{"duration":0.024154,"end_time":"2023-04-17T14:26:39.023019","exception":false,"start_time":"2023-04-17T14:26:38.998865","status":"completed"},"tags":[]},"outputs":[],"source":["def download_xml(date, current_time):\n","    \n","    global file_exist\n","    global empty_file\n","    \n","    file_exist = False\n","    empty_file = False\n","\n","    url = f'https://tisvcloud.freeway.gov.tw/history/motc20/VD/{date}/VDLive_{current_time}.xml.gz'\n","    filename = f'VDLive_{current_time}.xml.gz'\n","\n","    # Download the file\n","    if os.path.isfile(filename):\n","        if os.path.getsize(filename) > 0 and os.path.getsize(filename) < 1024:\n","            os.remove(filename)\n","            empty_file = True\n","            file_exist = True\n","        elif os.path.getsize(filename) == 0:\n","            os.remove(filename)\n","            file_exist = False\n","        else: file_exist = True\n","    else:\n","        urllib.request.urlretrieve(url, filename)\n","        if urllib.request.urlopen(url).getcode() == 404:\n","            file_exist = False\n","            os.remove(filename)\n","            return\n","        elif os.path.getsize(filename) > 0 and os.path.getsize(filename) < 1024:\n","            os.remove(filename)\n","            empty_file = True\n","            file_exist = True\n","            return\n","        elif os.path.getsize(filename) == 0:\n","            os.remove(filename)\n","            file_exist = False\n","            return\n","        else: file_exist = True\n","\n","    if file_exist and not empty_file:\n","        # File decompression\n","        if os.path.isfile(filename):\n","            with gzip.open(filename, 'rb') as f:\n","                xml_data = f.read()\n","\n","            xml_file_path = f'VDLive_{current_time}.xml'\n","\n","            with open(xml_file_path, 'wb') as f:\n","                f.write(xml_data)\n","\n","            if os.path.exists(xml_file_path):\n","                os.remove(filename)\n","            else:\n","                print(f'Error: {xml_file_path} was not created.')\n","        else:\n","            print(f\"Error: {filename} not found.\")"]},{"cell_type":"markdown","id":"f80cbb50","metadata":{"papermill":{"duration":0.004288,"end_time":"2023-04-17T14:26:39.03192","exception":false,"start_time":"2023-04-17T14:26:39.027632","status":"completed"},"tags":[]},"source":["## XML to JSON to DataFrame with pivoting to CSV pipeline"]},{"cell_type":"code","execution_count":4,"id":"2fa07d2f","metadata":{"execution":{"iopub.execute_input":"2023-04-17T14:26:39.043611Z","iopub.status.busy":"2023-04-17T14:26:39.04318Z","iopub.status.idle":"2023-04-17T14:26:39.078117Z","shell.execute_reply":"2023-04-17T14:26:39.076655Z"},"papermill":{"duration":0.044144,"end_time":"2023-04-17T14:26:39.080701","exception":false,"start_time":"2023-04-17T14:26:39.036557","status":"completed"},"tags":[]},"outputs":[],"source":["def get_vd(date, time, desired_vdid, realtime, dataset):\n","\n","    global vdid_found\n","    global vdid_absent\n","    vdid_found = False\n","    \n","\n","    if realtime:\n","        directory = f\"VDLive_{time}.xml\"\n","    else:\n","        global file_exist\n","        global empty_file\n","        global file_absent_in_dataset\n","        empty_file = False\n","        file_exist = False\n","        directory = f\"/kaggle/input/{dataset}/{date}/kaggle/working/{date}/VDLive_{time}.xml/VDLive_{time}.xml\"\n","        if not os.path.isfile(directory):\n","            file_absent_in_dataset.append(time)\n","            return\n","\n","        elif os.path.getsize(directory) > 0 and os.path.getsize(directory) < 1024:\n","            empty_file = True\n","            file_exist = True\n","            return\n","        elif os.path.getsize(directory) == 0:\n","            file_exist = False\n","            return\n","        else:\n","            file_exist = True\n","    #print(file_exist, empty_file)\n","    \n","    if file_exist and not empty_file:\n","        with open(directory) as xml_file:   # Parse XML data into a dictionary\n","            data_dict = xmltodict.parse(xml_file.read())\n","        json_data = json.dumps(data_dict) # Convert dictionary to JSON\n","        #print(\"XML file converted to JSON.\")    \n","        data = json.loads(json_data) # Load the JSON data into a dictionary\n","        #print(data)\n","        # Get the desired VDID, and iterate through the VDLives and find the one with the desired VDID\n","        for vdlive in data[\"VDLiveList\"][\"VDLives\"][\"VDLive\"]:\n","            if vdlive[\"VDID\"] == desired_vdid:\n","                # Extract the data for the desired VDID\n","                link_flows = vdlive[\"LinkFlows\"][\"LinkFlow\"]\n","                status = vdlive[\"Status\"]\n","                date_time = f'{date}_{time}'\n","                # Print the VDID and its corresponding data\n","                data= {\"VDID\": desired_vdid, \"LinkFlows\": link_flows, \"Status\": status,\n","                       #\"DateTime\": date_time\n","                      }\n","                #print(f\"Detector {desired_vdid} found.\")\n","                #print(data)\n","                vdid_found = True\n","                break\n","\n","        if vdid_found:\n","            # Extract data from JSON and flatten into a list of rows\n","            rows = []\n","            vdid = data['VDID']\n","            date_time = f'{date}_{time}'\n","            link_id = data['LinkFlows']['LinkID']\n","\n","            # Situation 1: 'Lane' key is present in the 'Lanes' dictionary\n","            if 'Lane' in data['LinkFlows']['Lanes']:\n","                lanes = data['LinkFlows']['Lanes']['Lane']\n","                if isinstance(lanes, dict):\n","                    # If 'Lane' is a dictionary, convert it to a list with a single element\n","                    lanes = [lanes]\n","                for lane in lanes:\n","                    lane_id = lane['LaneID']\n","                    lane_type = lane['LaneType']\n","                    lane_speed = lane['Speed']\n","                    occupancy = lane['Occupancy']\n","                    for vehicle in lane['Vehicles']['Vehicle']:\n","                        vehicle_type = vehicle['VehicleType']\n","                        volume = vehicle['Volume']\n","                        speed = vehicle['Speed']\n","                        rows.append([date_time, vdid, link_id, lane_id,\n","                                     lane_type, lane_speed, occupancy, \n","                                     vehicle_type, volume, speed])\n","\n","            # Situation 2: 'Lane' key is not present in the 'Lanes' dictionary\n","            elif 'Lane' in data['LinkFlows']['Lanes']['Lane']:\n","                lane = data['LinkFlows']['Lanes']['Lane']\n","                lane_id = lane['LaneID']\n","                lane_type = lane['LaneType']\n","                lane_speed = lane['Speed']\n","                occupancy = lane['Occupancy']\n","                for vehicle in lane['Vehicles']['Vehicle']:\n","                    vehicle_type = vehicle['VehicleType']\n","                    volume = vehicle['Volume']\n","                    speed = vehicle['Speed']\n","                    rows.append([date_time, vdid, link_id, lane_id,\n","                                 lane_type, lane_speed, occupancy,\n","                                 vehicle_type, volume, speed])\n","            return rows\n","        else:\n","            vdid_absent.append(str(time))\n","\n","\n","def get_vds(date, desired_vdid, start_time, end_time, realtime, dataset):\n","    \n","    global email_text\n","\n","    # Convert start_time and end_time to integers\n","    start_time2 = int(start_time)\n","    str_end_time = end_time\n","    end_time = int(end_time)\n","\n","    # Convert start_time and end_time to minutes\n","    start_time_minutes = (start_time2 // 100) * 60 + (start_time2 % 100)\n","    end_time_minutes = (end_time // 100) * 60 + (end_time % 100)\n","    total = end_time_minutes - start_time_minutes + 1\n","\n","    # Create an empty list to store the results\n","    results = []\n","    missing = []\n","    empty_files =[]\n","    file_absent_in_dataset =[]\n","    \n","    i = 0\n","    current_time = start_time\n","\n","    # Loop through the function multiple times and append the result to the list\n","    with tqdm(total=total) as pbar:\n","        while i < total:\n","            \n","            if realtime == True:\n","                download_xml(date, current_time)\n","            result = get_vd(date, current_time, desired_vdid, realtime, dataset) ###\n","            if vdid_found == True:\n","                results.extend(result)\n","            filename = f\"/kaggle/working/VDLive_{current_time}.xml\"\n","            if os.path.exists(filename):\n","                os.remove(filename)               \n","            if empty_file:\n","                empty_files.append(str(current_time))\n","            if not file_exist:\n","                missing.append(str(current_time))\n","                \n","            pbar.update(1)\n","            i += 1\n","\n","            if int(current_time) % 100 == 59:\n","                current_time = str(int(current_time)+41)\n","            else:\n","                current_time = str(int(current_time)+1)\n","            \n","            if int(current_time)<10:\n","                current_time = f\"000{current_time}\"\n","            elif int(current_time)<100:\n","                current_time = f\"00{current_time}\"\n","            elif int(current_time)<1000:\n","                current_time = f\"0{current_time}\"\n","            else:\n","                current_time = current_time\n","\n","    #print(results)\n","\n","    # Create a pandas DataFrame from the rows list\n","    df = pd.DataFrame(results, columns=['DateTime', 'VDID', 'LinkID',\n","                                        'LaneID', 'LaneType',\n","                                        'LaneSpeed', 'Occupancy',\n","                                        'VehicleType', 'Volume', 'Speed'])\n","    #pivot dataframe according to vehicle\n","    df = df.pivot(index=['DateTime', 'VDID', 'LinkID', 'LaneID',\n","                     'LaneType', 'LaneSpeed', 'Occupancy'],\n","              columns='VehicleType',\n","              values=['Volume', 'Speed'])\n","    df.columns = ['_'.join(col).strip() for col in df.columns.values]\n","    df = df.reset_index()\n","    df = df.reindex(columns=['DateTime', 'VDID', #'LinkID', \n","                             'LaneID',#'LaneType',\n","                             'LaneSpeed', 'Occupancy',\n","                             'Volume_S', 'Speed_S', 'Volume_L', 'Speed_L', 'Volume_T', 'Speed_T'])\n","    \n","    #pivot the dataframe according to lane\n","    df = df.pivot_table(index=['DateTime', 'VDID'], columns='LaneID', values=['LaneSpeed', 'Occupancy', 'Volume_S', 'Speed_S', 'Volume_L', 'Speed_L', 'Volume_T', 'Speed_T']).reset_index()\n","    #flatten the column names\n","    df.columns = ['_'.join(str(col).strip() for col in tup) for tup in df.columns.values]\n","\n","    #Save to CSV\n","    filename = f'{date}_{desired_vdid}_{start_time}_{str_end_time}'\n","    df.to_csv(f'{filename}.csv', index=False)\n","    \n","    \n","    report_string = (f\"{filename}.csv has been saved in /kaggle/working.\\n\"\n","      f\"missing files = {missing}\\n\"\n","      f\"files not found in dataset = {file_absent_in_dataset}\\n\"\n","      f\"empty files = {empty_files}\\n\"\n","      f\"VDID not found in files = {vdid_absent}\\n\"\n","      f\"\\n\")\n","    \n","    # Append the report string to the email text using a list\n","    email_text += report_string\n","    \n","    print(report_string)\n","    #display(df)"]},{"cell_type":"markdown","id":"5cfeaae0","metadata":{"papermill":{"duration":0.004248,"end_time":"2023-04-17T14:26:39.08961","exception":false,"start_time":"2023-04-17T14:26:39.085362","status":"completed"},"tags":[]},"source":["## Acquire time so the script does automatic daily updates for yesterday."]},{"cell_type":"code","execution_count":5,"id":"7440db94","metadata":{"execution":{"iopub.execute_input":"2023-04-17T14:26:39.101189Z","iopub.status.busy":"2023-04-17T14:26:39.100741Z","iopub.status.idle":"2023-04-17T14:26:39.149014Z","shell.execute_reply":"2023-04-17T14:26:39.147609Z"},"papermill":{"duration":0.057369,"end_time":"2023-04-17T14:26:39.151542","exception":false,"start_time":"2023-04-17T14:26:39.094173","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["20230416\n"]}],"source":["# Set the time zone to GMT+8\n","tz = pytz.timezone('Asia/Taipei')\n","# Get the current time in GMT+8 time zone\n","now = datetime.now(tz)\n","# Subtract one day to get yesterday's date\n","yesterday = now - timedelta(days=1)\n","# Format yesterday's date as YYYYMMDD\n","yesterday_str = yesterday.strftime('%Y%m%d')\n","\n","if use_yesterday== True:\n","    date_list = [yesterday_str]\n","\n","print(yesterday_str)\n","\n"]},{"cell_type":"markdown","id":"ea772b59","metadata":{"papermill":{"duration":0.004243,"end_time":"2023-04-17T14:26:39.160437","exception":false,"start_time":"2023-04-17T14:26:39.156194","status":"completed"},"tags":[]},"source":["# Mail execution results and data to the user with Gmail API."]},{"cell_type":"code","execution_count":6,"id":"8216db7a","metadata":{"execution":{"iopub.execute_input":"2023-04-17T14:26:39.17151Z","iopub.status.busy":"2023-04-17T14:26:39.171114Z","iopub.status.idle":"2023-04-17T14:26:39.187818Z","shell.execute_reply":"2023-04-17T14:26:39.186608Z"},"papermill":{"duration":0.02567,"end_time":"2023-04-17T14:26:39.190653","exception":false,"start_time":"2023-04-17T14:26:39.164983","status":"completed"},"tags":[]},"outputs":[],"source":["import smtplib\n","import os\n","from email.mime.text import MIMEText\n","from email.mime.multipart import MIMEMultipart\n","from email.mime.application import MIMEApplication\n","\n","def email_report(your_email):\n","\n","    ## Gmail account credentials\n","    gmail_user = 'galen147258369@gmail.com'\n","    gmail_password = 'ikikvtbtlecywjtt'\n","\n","    # Recipient email address\n","    to = your_email\n","\n","    # Create a multipart message container\n","    msg = MIMEMultipart()\n","\n","    # Set the subject of the email\n","    msg['Subject'] = 'Taiwan Highway VD project'\n","\n","    # Set the sender and recipient of the email\n","    msg['From'] = gmail_user\n","    msg['To'] = to\n","\n","    # Create a text message part\n","    text = MIMEText(email_text)\n","    msg.attach(text)\n","\n","    # Attach all files in the working directory to the email\n","    working_dir = '/kaggle/working/'\n","    file_list = [f for f in os.listdir(working_dir) if not f.startswith('.virtual_documents') and os.path.isfile(os.path.join(working_dir, f))]\n","    for filename in file_list:\n","        file_path = os.path.join(working_dir, filename)\n","        with open(file_path, 'rb') as f:\n","            file_data = f.read()\n","        attachment = MIMEApplication(file_data, _subtype='csv')\n","        attachment.add_header('content-disposition', 'attachment', filename=filename)\n","        msg.attach(attachment)\n","\n","    # Connect to Gmail's SMTP server and send the email\n","    server = smtplib.SMTP('smtp.gmail.com', 587)\n","    server.ehlo()\n","    server.starttls()\n","    server.login(gmail_user, gmail_password)\n","    server.sendmail(msg['From'], msg['To'], msg.as_string())\n","    server.quit()\n"]},{"cell_type":"markdown","id":"00d478f2","metadata":{"papermill":{"duration":0.004233,"end_time":"2023-04-17T14:26:39.199683","exception":false,"start_time":"2023-04-17T14:26:39.19545","status":"completed"},"tags":[]},"source":["# RUN THE CODE HERE !"]},{"cell_type":"code","execution_count":7,"id":"191377c5","metadata":{"execution":{"iopub.execute_input":"2023-04-17T14:26:39.210719Z","iopub.status.busy":"2023-04-17T14:26:39.2103Z","iopub.status.idle":"2023-04-17T19:56:56.50978Z","shell.execute_reply":"2023-04-17T19:56:56.50847Z"},"papermill":{"duration":19817.309027,"end_time":"2023-04-17T19:56:56.513211","exception":false,"start_time":"2023-04-17T14:26:39.204184","status":"completed"},"scrolled":true,"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset list: ['vd-2023-0326-0401']\n","Use all subfolders: False\n","Date list: ['20230416']\n","VDID list: ['VD-N1-S-369.007-M-Loop', 'VD-N1-S-369.400-M-Loop', 'VD-N1-S-370.000-M-Loop', 'VD-N1-S-371.010-M-Loop']\n","Start time: 0000\n","End time: 2359\n","Realtime: True\n","Your email: galen147258369@gmail.com\n","\n","\n"," Total tasks: 4\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1440/1440 [1:23:12<00:00,  3.47s/it]\n"]},{"name":"stdout","output_type":"stream","text":["20230416_VD-N1-S-369.007-M-Loop_0000_2359.csv has been saved in /kaggle/working.\n","missing files = []\n","files not found in dataset = []\n","empty files = []\n","VDID not found in files = []\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1440/1440 [1:22:12<00:00,  3.43s/it]\n"]},{"name":"stdout","output_type":"stream","text":["20230416_VD-N1-S-369.400-M-Loop_0000_2359.csv has been saved in /kaggle/working.\n","missing files = []\n","files not found in dataset = []\n","empty files = []\n","VDID not found in files = []\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1440/1440 [1:22:12<00:00,  3.43s/it]\n"]},{"name":"stdout","output_type":"stream","text":["20230416_VD-N1-S-370.000-M-Loop_0000_2359.csv has been saved in /kaggle/working.\n","missing files = []\n","files not found in dataset = []\n","empty files = []\n","VDID not found in files = []\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1440/1440 [1:22:13<00:00,  3.43s/it]\n"]},{"name":"stdout","output_type":"stream","text":["20230416_VD-N1-S-371.010-M-Loop_0000_2359.csv has been saved in /kaggle/working.\n","missing files = []\n","files not found in dataset = []\n","empty files = []\n","VDID not found in files = []\n","\n","\n","Email sent.\n"]}],"source":["if clear_working_folder == True:\n","    !rm -rf /kaggle/working/*\n","    \n","if len(dataset_list) > 1:\n","    use_all_subfolders == True\n","    realtime == False\n","\n","file_exist = False\n","empty_file = False\n","vdid_found = False\n","file_absent_in_dataset = []\n","vdid_absent = []\n","email_text = \"\"\n","task = 0\n","\n","report_string = \"Dataset list: \" + str(dataset_list) + \"\\n\"\n","report_string += \"Use all subfolders: \" + str(use_all_subfolders) + \"\\n\"\n","report_string += \"Date list: \" + str(date_list) + \"\\n\"\n","report_string += \"VDID list: \" + str(vdid_list) + \"\\n\"\n","report_string += \"Start time: \" + start_time + \"\\n\"\n","report_string += \"End time: \" + end_time + \"\\n\"\n","report_string += \"Realtime: \" + str(realtime) + \"\\n\"\n","report_string += \"Your email: \" + your_email + \"\\n\"\n","report_string += \"\\n\"\n","\n","print(report_string)\n","\n","email_text += report_string\n","\n","total_tasks = 0\n","\n","if use_all_subfolders:\n","    subfolders = []\n","    for dataset in dataset_list:\n","        for folder_name in os.listdir(f'/kaggle/input/{dataset}'):\n","            if os.path.isdir(os.path.join(f'/kaggle/input/{dataset}', folder_name)):\n","                subfolders.append(folder_name)\n","        date_list = subfolders\n","        total_tasks += len(date_list)*len(vdid_list)\n","else:\n","    total_tasks = len(date_list)*len(vdid_list)\n","    \n","print(f' Total tasks: {total_tasks}')\n","print()\n","\n","task = 0\n","\n","if use_all_subfolders:\n","    realtime = False # if use all subfolders, then realtime must be false.\n","    for dataset in dataset_list:\n","        subfolders = []\n","        for folder_name in os.listdir(f'/kaggle/input/{dataset}'):\n","            if os.path.isdir(os.path.join(f'/kaggle/input/{dataset}', folder_name)):\n","                subfolders.append(folder_name)\n","        date_list = subfolders\n","        for date in date_list:\n","            for vdid in vdid_list:\n","                get_vds(date, vdid, start_time, end_time, realtime, dataset)\n","                task += 1\n","else:\n","    for date in date_list:\n","        for vdid in vdid_list:\n","            get_vds(date, vdid, start_time, end_time, realtime, dataset_list[0])\n","            task += 1\n","\n","loop_complete = f\"{task} out of {total_tasks} tasks in the loop were completed.\"\n","\n","email_text +=loop_complete\n","email_text += \"\\n\"\n","email_text += \"\\n\"\n","\n","# Get a list of all files in the /kaggle/working/ directory excluding '.virtual_documents' directory\n","file_list = [f for f in os.listdir('/kaggle/working/') if not f.startswith('.virtual_documents') and os.path.isfile(os.path.join('/kaggle/working/', f))]\n","# Add the list of files to email_text\n","email_text += \"\\n\".join([f\"Attachment: {filename}\" for filename in file_list]) + \"\\n\"\n","\n","email_text += \"\\n\"\n","email_text += \"END OF MESSAGE\"\n","\n","email_report(your_email)\n","print(\"Email sent.\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":19842.967757,"end_time":"2023-04-17T19:56:57.878402","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-04-17T14:26:14.910645","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}