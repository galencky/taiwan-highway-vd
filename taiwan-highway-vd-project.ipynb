{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/galenchen/taiwan-highway-vd-project?scriptVersionId=126975610\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Download multiple DATE x VDID loop\n## Set parameters.","metadata":{}},{"cell_type":"code","source":"#***********************************************************************\nclear_working_folder = True # so you will only be mailed with the latest results, but make sure the files have been saved.\nuse_all_subfolders = False # if this is true, then you can only use Kaggle dataset.\nrealtime = True #will be set to false if you use all subfolders\nuse_yesterday = True # if true, the date_list will be changed to yesterday for scheduled runs.\n\ndataset_list = ['vd-2023-0326-0401'] # If the length of this list is longer than one, use_all_subfolders == True, realtime == False.\ndate_list = ['20230401']#[yesterday_str] if you want to run data from yesterday\n#vdid_list = [\"VD-N1-S-369.007-M-Loop\", \"VD-N1-S-369.400-M-Loop\", \"VD-N1-S-370.000-M-Loop\", \"VD-N1-S-371.010-M-Loop\"]\n#vdid_list = [\"VD-N1-S-370.000-M-Loop\", \"VD-N1-S-371.010-M-Loop\"]\nvdid_list = ['VD-N5-N-15.488-M-LOOP', 'VD-N5-N-16.196-M-LOOP', 'VD-N5-N-19.012-M-LOOP',\n             'VD-N5-N-19.689-PS-M-LOOP', 'VD-N5-N-27.468-M-LOOP', 'VD-N5-N-27.779-M-LOOP']\nstart_time = \"1000\"\nend_time = \"1003\"\n\nyour_email = 'tudo11927.y@nycu.edu.tw'\n#***********************************************************************","metadata":{"execution":{"iopub.status.busy":"2023-04-24T05:46:36.51425Z","iopub.execute_input":"2023-04-24T05:46:36.515325Z","iopub.status.idle":"2023-04-24T05:46:36.522496Z","shell.execute_reply.started":"2023-04-24T05:46:36.515279Z","shell.execute_reply":"2023-04-24T05:46:36.521178Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Load the dataset and the packages\n* Suggest uploading the xml.gz files onto Kaggle so it doesn't fuck up the output storage.\n* Also, the package xmltodict have to be installed before anything happens.","metadata":{}},{"cell_type":"code","source":"!pip install xmltodict # Install xmltodict package if not already installed\n\nimport xmltodict # Import xmltodict module for parsing XML data\nimport numpy as np # Import numpy and pandas for data analysis and processing\nimport pandas as pd\nimport json # Import json module for working with JSON data\nfrom tqdm import tqdm # Import tqdm module for progress bars\nimport urllib.request # Import urllib.request module for working with URLs\nimport gzip # Import gzip module for working with compressed data files\nfrom datetime import datetime, timedelta \nimport pytz # Import datetime and pytz modules for working with dates and times\nimport os # Import os module for working with the file system\n\n# Walk through the /kaggle/input directory and print out the path of each file in it\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-04-24T05:46:36.524541Z","iopub.execute_input":"2023-04-24T05:46:36.524878Z","iopub.status.idle":"2023-04-24T05:46:47.824675Z","shell.execute_reply.started":"2023-04-24T05:46:36.524846Z","shell.execute_reply":"2023-04-24T05:46:47.823127Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Requirement already satisfied: xmltodict in /opt/conda/lib/python3.7/site-packages (0.13.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"## Realtime downloading and conversion pipeline\n\nDownload xml.gz file, decompress into xml file, then delete the xml.gz, then feed the xml into get_vd, then delete the xml after data is analyzed.","metadata":{}},{"cell_type":"code","source":"def download_xml(date, current_time):\n    \n    global file_exist\n    global empty_file\n    \n    file_exist = False\n    empty_file = False\n\n    url = f'https://tisvcloud.freeway.gov.tw/history/motc20/VD/{date}/VDLive_{current_time}.xml.gz'\n    filename = f'VDLive_{current_time}.xml.gz'\n\n    # Download the file\n    if os.path.isfile(filename):\n        if os.path.getsize(filename) > 0 and os.path.getsize(filename) < 1024:\n            os.remove(filename)\n            empty_file = True\n            file_exist = True\n        elif os.path.getsize(filename) == 0:\n            os.remove(filename)\n            file_exist = False\n        else: file_exist = True\n    else:\n        urllib.request.urlretrieve(url, filename)\n        if urllib.request.urlopen(url).getcode() == 404:\n            file_exist = False\n            os.remove(filename)\n            return\n        elif os.path.getsize(filename) > 0 and os.path.getsize(filename) < 1024:\n            os.remove(filename)\n            empty_file = True\n            file_exist = True\n            return\n        elif os.path.getsize(filename) == 0:\n            os.remove(filename)\n            file_exist = False\n            return\n        else: file_exist = True\n\n    if file_exist and not empty_file:\n        # File decompression\n        if os.path.isfile(filename):\n            with gzip.open(filename, 'rb') as f:\n                xml_data = f.read()\n\n            xml_file_path = f'VDLive_{current_time}.xml'\n\n            with open(xml_file_path, 'wb') as f:\n                f.write(xml_data)\n\n            if os.path.exists(xml_file_path):\n                os.remove(filename)\n            else:\n                print(f'Error: {xml_file_path} was not created.')\n        else:\n            print(f\"Error: {filename} not found.\")","metadata":{"execution":{"iopub.status.busy":"2023-04-24T05:46:47.827128Z","iopub.execute_input":"2023-04-24T05:46:47.827551Z","iopub.status.idle":"2023-04-24T05:46:47.847516Z","shell.execute_reply.started":"2023-04-24T05:46:47.827508Z","shell.execute_reply":"2023-04-24T05:46:47.846192Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## XML to JSON to DataFrame with pivoting to CSV pipeline","metadata":{}},{"cell_type":"code","source":"def get_vd(date, time, desired_vdid, realtime, dataset):\n\n    global vdid_found\n    global vdid_absent\n    vdid_found = False\n    \n\n    if realtime:\n        directory = f\"VDLive_{time}.xml\"\n    else:\n        global file_exist\n        global empty_file\n        global file_absent_in_dataset\n        empty_file = False\n        file_exist = False\n        directory = f\"/kaggle/input/{dataset}/{date}/kaggle/working/{date}/VDLive_{time}.xml/VDLive_{time}.xml\"\n        if not os.path.isfile(directory):\n            file_absent_in_dataset.append(time)\n            return\n\n        elif os.path.getsize(directory) > 0 and os.path.getsize(directory) < 1024:\n            empty_file = True\n            file_exist = True\n            return\n        elif os.path.getsize(directory) == 0:\n            file_exist = False\n            return\n        else:\n            file_exist = True\n    #print(file_exist, empty_file)\n    \n    if file_exist and not empty_file:\n        with open(directory) as xml_file:   # Parse XML data into a dictionary\n            data_dict = xmltodict.parse(xml_file.read())\n        json_data = json.dumps(data_dict) # Convert dictionary to JSON\n        #print(\"XML file converted to JSON.\")    \n        data = json.loads(json_data) # Load the JSON data into a dictionary\n        #print(data)\n        # Get the desired VDID, and iterate through the VDLives and find the one with the desired VDID\n        for vdlive in data[\"VDLiveList\"][\"VDLives\"][\"VDLive\"]:\n            if vdlive[\"VDID\"] == desired_vdid:\n                # Extract the data for the desired VDID\n                link_flows = vdlive[\"LinkFlows\"][\"LinkFlow\"]\n                status = vdlive[\"Status\"]\n                date_time = f'{date}_{time}'\n                # Print the VDID and its corresponding data\n                data= {\"VDID\": desired_vdid, \"LinkFlows\": link_flows, \"Status\": status,\n                       #\"DateTime\": date_time\n                      }\n                #print(f\"Detector {desired_vdid} found.\")\n                #print(data)\n                vdid_found = True\n                break\n\n        if vdid_found:\n            # Extract data from JSON and flatten into a list of rows\n            rows = []\n            vdid = data['VDID']\n            date_time = f'{date}_{time}'\n            link_id = data['LinkFlows']['LinkID']\n\n            # Situation 1: 'Lane' key is present in the 'Lanes' dictionary\n            if 'Lane' in data['LinkFlows']['Lanes']:\n                lanes = data['LinkFlows']['Lanes']['Lane']\n                if isinstance(lanes, dict):\n                    # If 'Lane' is a dictionary, convert it to a list with a single element\n                    lanes = [lanes]\n                for lane in lanes:\n                    lane_id = lane['LaneID']\n                    lane_type = lane['LaneType']\n                    lane_speed = lane['Speed']\n                    occupancy = lane['Occupancy']\n                    for vehicle in lane['Vehicles']['Vehicle']:\n                        vehicle_type = vehicle['VehicleType']\n                        volume = vehicle['Volume']\n                        speed = vehicle['Speed']\n                        rows.append([date_time, vdid, link_id, lane_id,\n                                     lane_type, lane_speed, occupancy, \n                                     vehicle_type, volume, speed])\n\n            # Situation 2: 'Lane' key is not present in the 'Lanes' dictionary\n            elif 'Lane' in data['LinkFlows']['Lanes']['Lane']:\n                lane = data['LinkFlows']['Lanes']['Lane']\n                lane_id = lane['LaneID']\n                lane_type = lane['LaneType']\n                lane_speed = lane['Speed']\n                occupancy = lane['Occupancy']\n                for vehicle in lane['Vehicles']['Vehicle']:\n                    vehicle_type = vehicle['VehicleType']\n                    volume = vehicle['Volume']\n                    speed = vehicle['Speed']\n                    rows.append([date_time, vdid, link_id, lane_id,\n                                 lane_type, lane_speed, occupancy,\n                                 vehicle_type, volume, speed])\n            return rows\n        else:\n            vdid_absent.append(str(time))\n\n\ndef get_vds(date, desired_vdid, start_time, end_time, realtime, dataset):\n    \n    global email_text\n    global vdid_absent\n\n    # Convert start_time and end_time to integers\n    start_time2 = int(start_time)\n    str_end_time = end_time\n    end_time = int(end_time)\n\n    # Convert start_time and end_time to minutes\n    start_time_minutes = (start_time2 // 100) * 60 + (start_time2 % 100)\n    end_time_minutes = (end_time // 100) * 60 + (end_time % 100)\n    total = end_time_minutes - start_time_minutes + 1\n\n    # Create an empty list to store the results\n    results = []\n    missing = []\n    empty_files =[]\n    file_absent_in_dataset =[]\n    vdid_absent = []\n    \n    i = 0\n    current_time = start_time\n\n    # Loop through the function multiple times and append the result to the list\n    with tqdm(total=total) as pbar:\n        while i < total:\n            \n            if realtime == True:\n                download_xml(date, current_time)\n            result = get_vd(date, current_time, desired_vdid, realtime, dataset) ###\n            if vdid_found == True:\n                results.extend(result)\n            filename = f\"/kaggle/working/VDLive_{current_time}.xml\"\n            if os.path.exists(filename):\n                os.remove(filename)               \n            if empty_file:\n                empty_files.append(str(current_time))\n            if not file_exist:\n                missing.append(str(current_time))\n                \n            pbar.update(1)\n            i += 1\n\n            if int(current_time) % 100 == 59:\n                current_time = str(int(current_time)+41)\n            else:\n                current_time = str(int(current_time)+1)\n            \n            if int(current_time)<10:\n                current_time = f\"000{current_time}\"\n            elif int(current_time)<100:\n                current_time = f\"00{current_time}\"\n            elif int(current_time)<1000:\n                current_time = f\"0{current_time}\"\n            else:\n                current_time = current_time\n\n    #print(results)\n\n    # Create a pandas DataFrame from the rows list\n    df = pd.DataFrame(results, columns=['DateTime', 'VDID', 'LinkID',\n                                        'LaneID', 'LaneType',\n                                        'LaneSpeed', 'Occupancy',\n                                        'VehicleType', 'Volume', 'Speed'])\n    #pivot dataframe according to vehicle\n    df = df.pivot(index=['DateTime', 'VDID', 'LinkID', 'LaneID',\n                     'LaneType', 'LaneSpeed', 'Occupancy'],\n              columns='VehicleType',\n              values=['Volume', 'Speed'])\n    df.columns = ['_'.join(col).strip() for col in df.columns.values]\n    df = df.reset_index()\n    df = df.reindex(columns=['DateTime', 'VDID', #'LinkID', \n                             'LaneID',#'LaneType',\n                             'LaneSpeed', 'Occupancy',\n                             'Volume_S', 'Speed_S', 'Volume_L', 'Speed_L', 'Volume_T', 'Speed_T'])\n    \n    #pivot the dataframe according to lane\n    df = df.pivot_table(index=['DateTime', 'VDID'], columns='LaneID', values=['LaneSpeed', 'Occupancy', 'Volume_S', 'Speed_S', 'Volume_L', 'Speed_L', 'Volume_T', 'Speed_T']).reset_index()\n    #flatten the column names\n    df.columns = ['_'.join(str(col).strip() for col in tup) for tup in df.columns.values]\n\n    #Save to CSV\n    filename = f'{date}_{desired_vdid}_{start_time}_{str_end_time}'\n    df.to_csv(f'{filename}.csv', index=False)\n    \n    \n    report_string = (f\"{filename}.csv has been saved in /kaggle/working.\\n\"\n      f\"missing files = {missing}\\n\"\n      f\"files not found in dataset = {file_absent_in_dataset}\\n\"\n      f\"empty files = {empty_files}\\n\"\n      f\"VDID not found in files = {vdid_absent}\\n\"\n      f\"\\n\")\n    \n    # Append the report string to the email text using a list\n    email_text += report_string\n    \n    print(report_string)\n    #display(df)","metadata":{"execution":{"iopub.status.busy":"2023-04-24T05:46:47.850272Z","iopub.execute_input":"2023-04-24T05:46:47.850657Z","iopub.status.idle":"2023-04-24T05:46:47.889252Z","shell.execute_reply.started":"2023-04-24T05:46:47.85062Z","shell.execute_reply":"2023-04-24T05:46:47.887898Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Acquire time so the script does automatic daily updates for yesterday.","metadata":{}},{"cell_type":"code","source":"# Set the time zone to GMT+8\ntz = pytz.timezone('Asia/Taipei')\n# Get the current time in GMT+8 time zone\nnow = datetime.now(tz)\n# Subtract one day to get yesterday's date\nyesterday = now - timedelta(days=1)\n# Format yesterday's date as YYYYMMDD\nyesterday_str = yesterday.strftime('%Y%m%d')\n\nif use_yesterday== True:\n    date_list = [yesterday_str]\n\nprint(yesterday_str)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-24T05:46:47.891059Z","iopub.execute_input":"2023-04-24T05:46:47.892534Z","iopub.status.idle":"2023-04-24T05:46:47.906309Z","shell.execute_reply.started":"2023-04-24T05:46:47.89247Z","shell.execute_reply":"2023-04-24T05:46:47.904928Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"20230423\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Mail execution results and data to the user with Gmail API.","metadata":{}},{"cell_type":"code","source":"import smtplib\nimport os\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.application import MIMEApplication\n\ndef email_report(your_email):\n\n    ## Gmail account credentials\n    gmail_user = 'galen147258369@gmail.com'\n    gmail_password = 'ikikvtbtlecywjtt'\n\n    # Recipient email address\n    to = your_email\n\n    # Create a multipart message container\n    msg = MIMEMultipart()\n\n    # Set the subject of the email\n    msg['Subject'] = 'Taiwan Highway VD project'\n\n    # Set the sender and recipient of the email\n    msg['From'] = gmail_user\n    msg['To'] = to\n\n    # Create a text message part\n    text = MIMEText(email_text)\n    msg.attach(text)\n\n    # Attach all files in the working directory to the email\n    working_dir = '/kaggle/working/'\n    file_list = [f for f in os.listdir(working_dir) if not f.startswith('.virtual_documents') and os.path.isfile(os.path.join(working_dir, f))]\n    for filename in file_list:\n        file_path = os.path.join(working_dir, filename)\n        with open(file_path, 'rb') as f:\n            file_data = f.read()\n        attachment = MIMEApplication(file_data, _subtype='csv')\n        attachment.add_header('content-disposition', 'attachment', filename=filename)\n        msg.attach(attachment)\n\n    # Connect to Gmail's SMTP server and send the email\n    server = smtplib.SMTP('smtp.gmail.com', 587)\n    server.ehlo()\n    server.starttls()\n    server.login(gmail_user, gmail_password)\n    server.sendmail(msg['From'], msg['To'], msg.as_string())\n    server.quit()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-24T05:46:47.910353Z","iopub.execute_input":"2023-04-24T05:46:47.910707Z","iopub.status.idle":"2023-04-24T05:46:47.923358Z","shell.execute_reply.started":"2023-04-24T05:46:47.910675Z","shell.execute_reply":"2023-04-24T05:46:47.921792Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# RUN THE CODE HERE !","metadata":{}},{"cell_type":"code","source":"if clear_working_folder == True:\n    !rm -rf /kaggle/working/*\n    \nif len(dataset_list) > 1:\n    use_all_subfolders == True\n    realtime == False\n\nfile_exist = False\nempty_file = False\nvdid_found = False\nfile_absent_in_dataset = []\nvdid_absent = []\nemail_text = \"\"\ntask = 0\n\nreport_string = \"Dataset list: \" + str(dataset_list) + \"\\n\"\nreport_string += \"Use all subfolders: \" + str(use_all_subfolders) + \"\\n\"\nreport_string += \"Date list: \" + str(date_list) + \"\\n\"\nreport_string += \"VDID list: \" + str(vdid_list) + \"\\n\"\nreport_string += \"Start time: \" + start_time + \"\\n\"\nreport_string += \"End time: \" + end_time + \"\\n\"\nreport_string += \"Realtime: \" + str(realtime) + \"\\n\"\nreport_string += \"Your email: \" + your_email + \"\\n\"\nreport_string += \"\\n\"\n\nprint(report_string)\n\nemail_text += report_string\n\ntotal_tasks = 0\n\nif use_all_subfolders:\n    subfolders = []\n    for dataset in dataset_list:\n        for folder_name in os.listdir(f'/kaggle/input/{dataset}'):\n            if os.path.isdir(os.path.join(f'/kaggle/input/{dataset}', folder_name)):\n                subfolders.append(folder_name)\n        date_list = subfolders\n        total_tasks += len(date_list)*len(vdid_list)\nelse:\n    total_tasks = len(date_list)*len(vdid_list)\n    \nprint(f' Total tasks: {total_tasks}')\nprint()\n\ntask = 0\n\nif use_all_subfolders:\n    realtime = False # if use all subfolders, then realtime must be false.\n    for dataset in dataset_list:\n        subfolders = []\n        for folder_name in os.listdir(f'/kaggle/input/{dataset}'):\n            if os.path.isdir(os.path.join(f'/kaggle/input/{dataset}', folder_name)):\n                subfolders.append(folder_name)\n        date_list = subfolders\n        for date in date_list:\n            for vdid in vdid_list:\n                get_vds(date, vdid, start_time, end_time, realtime, dataset)\n                task += 1\nelse:\n    for date in date_list:\n        for vdid in vdid_list:\n            get_vds(date, vdid, start_time, end_time, realtime, dataset_list[0])\n            task += 1\n\nloop_complete = f\"{task} out of {total_tasks} tasks in the loop were completed.\"\n\nemail_text +=loop_complete\nemail_text += \"\\n\"\nemail_text += \"\\n\"\n\n# Get a list of all files in the /kaggle/working/ directory excluding '.virtual_documents' directory\nfile_list = [f for f in os.listdir('/kaggle/working/') if not f.startswith('.virtual_documents') and os.path.isfile(os.path.join('/kaggle/working/', f))]\n# Add the list of files to email_text\nemail_text += \"\\n\".join([f\"Attachment: {filename}\" for filename in file_list]) + \"\\n\"\n\nemail_text += \"\\n\"\nemail_text += \"END OF MESSAGE\"\n\nemail_report(your_email)\nprint(\"Email sent.\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-04-24T05:46:47.925218Z","iopub.execute_input":"2023-04-24T05:46:47.925708Z","iopub.status.idle":"2023-04-24T05:48:26.533988Z","shell.execute_reply.started":"2023-04-24T05:46:47.925656Z","shell.execute_reply":"2023-04-24T05:48:26.532652Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Dataset list: ['vd-2023-0326-0401']\nUse all subfolders: False\nDate list: ['20230423']\nVDID list: ['VD-N5-N-15.488-M-LOOP', 'VD-N5-N-16.196-M-LOOP', 'VD-N5-N-19.012-M-LOOP', 'VD-N5-N-19.689-PS-M-LOOP', 'VD-N5-N-27.468-M-LOOP', 'VD-N5-N-27.779-M-LOOP']\nStart time: 1000\nEnd time: 1003\nRealtime: True\nYour email: tudo11927.y@nycu.edu.tw\n\n\n Total tasks: 6\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:16<00:00,  4.15s/it]\n","output_type":"stream"},{"name":"stdout","text":"20230423_VD-N5-N-15.488-M-LOOP_1000_1003.csv has been saved in /kaggle/working.\nmissing files = []\nfiles not found in dataset = []\nempty files = []\nVDID not found in files = []\n\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:16<00:00,  4.08s/it]\n","output_type":"stream"},{"name":"stdout","text":"20230423_VD-N5-N-16.196-M-LOOP_1000_1003.csv has been saved in /kaggle/working.\nmissing files = []\nfiles not found in dataset = []\nempty files = []\nVDID not found in files = []\n\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:15<00:00,  3.95s/it]\n","output_type":"stream"},{"name":"stdout","text":"20230423_VD-N5-N-19.012-M-LOOP_1000_1003.csv has been saved in /kaggle/working.\nmissing files = []\nfiles not found in dataset = []\nempty files = []\nVDID not found in files = []\n\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:15<00:00,  3.94s/it]\n","output_type":"stream"},{"name":"stdout","text":"20230423_VD-N5-N-19.689-PS-M-LOOP_1000_1003.csv has been saved in /kaggle/working.\nmissing files = []\nfiles not found in dataset = []\nempty files = []\nVDID not found in files = ['1000', '1001', '1002', '1003']\n\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:16<00:00,  4.00s/it]\n","output_type":"stream"},{"name":"stdout","text":"20230423_VD-N5-N-27.468-M-LOOP_1000_1003.csv has been saved in /kaggle/working.\nmissing files = []\nfiles not found in dataset = []\nempty files = []\nVDID not found in files = []\n\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [00:15<00:00,  3.94s/it]\n","output_type":"stream"},{"name":"stdout","text":"20230423_VD-N5-N-27.779-M-LOOP_1000_1003.csv has been saved in /kaggle/working.\nmissing files = []\nfiles not found in dataset = []\nempty files = []\nVDID not found in files = []\n\n\nEmail sent.\n","output_type":"stream"}]}]}